{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import urllib\n",
    "import bs4 as bs\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models import word2vec\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import logging\n",
    "from multiprocessing import cpu_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaração\n",
    "STOP_WORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(v_text):\n",
    "    v_text = [word.lower() for word in v_text]\n",
    "    v_text = [word for word in v_text if word not in STOP_WORDS]\n",
    "        \n",
    "    return v_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_records': 1701, 'record_format': 'list of str (tokens)', 'file_size': 33182058, 'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py', 'license': 'not found', 'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.', 'checksum': '68799af40b6bda07dfa47a32612e5364', 'file_name': 'text8.gz', 'read_more': ['http://mattmahoney.net/dc/textdata.html'], 'parts': 1}\n"
     ]
    }
   ],
   "source": [
    "print(api.info('text8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = api.load(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =[]\n",
    "for word in dataset:\n",
    "    #print(clean_text(word))\n",
    "    #print((word))\n",
    "    #break\n",
    "    data.append(clean_text(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 00:59:28,520 : INFO : collecting all words and their counts\n",
      "2021-11-30 00:59:28,521 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-11-30 00:59:29,897 : INFO : collected 253702 word types from a corpus of 10890638 raw words and 1701 sentences\n",
      "2021-11-30 00:59:29,898 : INFO : Loading a fresh vocabulary\n",
      "2021-11-30 00:59:30,830 : INFO : effective_min_count=0 retains 253702 unique words (100% of original 253702, drops 0)\n",
      "2021-11-30 00:59:30,831 : INFO : effective_min_count=0 leaves 10890638 word corpus (100% of original 10890638, drops 0)\n",
      "2021-11-30 00:59:31,269 : INFO : deleting the raw counts dictionary of 253702 items\n",
      "2021-11-30 00:59:31,274 : INFO : sample=0.001 downsamples 12 most-common words\n",
      "2021-11-30 00:59:31,275 : INFO : downsampling leaves estimated 9629320 word corpus (88.4% of prior 10890638)\n",
      "2021-11-30 00:59:31,728 : INFO : estimated required memory for 253702 words and 100 dimensions: 329812600 bytes\n",
      "2021-11-30 00:59:31,728 : INFO : resetting layer weights\n",
      "2021-11-30 00:59:33,310 : INFO : training model with 12 workers on 253702 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-11-30 00:59:34,315 : INFO : EPOCH 1 - PROGRESS: at 20.16% examples, 1922439 words/s, in_qsize 24, out_qsize 0\n",
      "2021-11-30 00:59:35,317 : INFO : EPOCH 1 - PROGRESS: at 40.33% examples, 1936453 words/s, in_qsize 23, out_qsize 1\n",
      "2021-11-30 00:59:36,317 : INFO : EPOCH 1 - PROGRESS: at 59.91% examples, 1923140 words/s, in_qsize 23, out_qsize 1\n",
      "2021-11-30 00:59:37,320 : INFO : EPOCH 1 - PROGRESS: at 78.78% examples, 1893126 words/s, in_qsize 23, out_qsize 1\n",
      "2021-11-30 00:59:38,322 : INFO : EPOCH 1 - PROGRESS: at 98.47% examples, 1894025 words/s, in_qsize 23, out_qsize 0\n",
      "2021-11-30 00:59:38,368 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-11-30 00:59:38,375 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-11-30 00:59:38,376 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-11-30 00:59:38,384 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-11-30 00:59:38,390 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-11-30 00:59:38,400 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-11-30 00:59:38,402 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-11-30 00:59:38,410 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-11-30 00:59:38,411 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-11-30 00:59:38,411 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-30 00:59:38,416 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-30 00:59:38,417 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-30 00:59:38,418 : INFO : EPOCH - 1 : training on 10890638 raw words (9629205 effective words) took 5.1s, 1886750 effective words/s\n",
      "2021-11-30 00:59:39,424 : INFO : EPOCH 2 - PROGRESS: at 19.99% examples, 1905619 words/s, in_qsize 21, out_qsize 2\n",
      "2021-11-30 00:59:40,434 : INFO : EPOCH 2 - PROGRESS: at 40.09% examples, 1917364 words/s, in_qsize 24, out_qsize 0\n",
      "2021-11-30 00:59:41,439 : INFO : EPOCH 2 - PROGRESS: at 59.85% examples, 1913129 words/s, in_qsize 22, out_qsize 1\n",
      "2021-11-30 00:59:42,449 : INFO : EPOCH 2 - PROGRESS: at 79.72% examples, 1905422 words/s, in_qsize 21, out_qsize 2\n",
      "2021-11-30 00:59:43,462 : INFO : EPOCH 2 - PROGRESS: at 97.53% examples, 1864211 words/s, in_qsize 23, out_qsize 0\n",
      "2021-11-30 00:59:43,556 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-11-30 00:59:43,557 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-11-30 00:59:43,558 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-11-30 00:59:43,564 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-11-30 00:59:43,574 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-11-30 00:59:43,583 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-11-30 00:59:43,583 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-11-30 00:59:43,584 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-11-30 00:59:43,584 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-11-30 00:59:43,587 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-30 00:59:43,595 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-30 00:59:43,596 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-30 00:59:43,597 : INFO : EPOCH - 2 : training on 10890638 raw words (9628924 effective words) took 5.2s, 1861176 effective words/s\n",
      "2021-11-30 00:59:44,607 : INFO : EPOCH 3 - PROGRESS: at 18.69% examples, 1776495 words/s, in_qsize 24, out_qsize 0\n",
      "2021-11-30 00:59:45,610 : INFO : EPOCH 3 - PROGRESS: at 38.27% examples, 1832761 words/s, in_qsize 22, out_qsize 1\n",
      "2021-11-30 00:59:46,617 : INFO : EPOCH 3 - PROGRESS: at 57.61% examples, 1843446 words/s, in_qsize 23, out_qsize 0\n",
      "2021-11-30 00:59:47,621 : INFO : EPOCH 3 - PROGRESS: at 77.54% examples, 1858023 words/s, in_qsize 23, out_qsize 0\n",
      "2021-11-30 00:59:48,621 : INFO : EPOCH 3 - PROGRESS: at 96.83% examples, 1858533 words/s, in_qsize 23, out_qsize 0\n",
      "2021-11-30 00:59:48,751 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-11-30 00:59:48,767 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-11-30 00:59:48,770 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-11-30 00:59:48,781 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-11-30 00:59:48,782 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-11-30 00:59:48,784 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-11-30 00:59:48,784 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-11-30 00:59:48,785 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-11-30 00:59:48,785 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-11-30 00:59:48,786 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-30 00:59:48,786 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-30 00:59:48,806 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-30 00:59:48,807 : INFO : EPOCH - 3 : training on 10890638 raw words (9629842 effective words) took 5.2s, 1850178 effective words/s\n",
      "2021-11-30 00:59:49,815 : INFO : EPOCH 4 - PROGRESS: at 20.16% examples, 1914532 words/s, in_qsize 24, out_qsize 0\n",
      "2021-11-30 00:59:50,816 : INFO : EPOCH 4 - PROGRESS: at 39.45% examples, 1890938 words/s, in_qsize 24, out_qsize 2\n",
      "2021-11-30 00:59:51,816 : INFO : EPOCH 4 - PROGRESS: at 54.79% examples, 1757346 words/s, in_qsize 24, out_qsize 0\n",
      "2021-11-30 00:59:52,826 : INFO : EPOCH 4 - PROGRESS: at 73.13% examples, 1753981 words/s, in_qsize 24, out_qsize 1\n",
      "2021-11-30 00:59:53,841 : INFO : EPOCH 4 - PROGRESS: at 92.71% examples, 1775063 words/s, in_qsize 23, out_qsize 2\n",
      "2021-11-30 00:59:54,183 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-11-30 00:59:54,183 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-11-30 00:59:54,190 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-11-30 00:59:54,190 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-11-30 00:59:54,196 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-11-30 00:59:54,207 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-11-30 00:59:54,209 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-11-30 00:59:54,211 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-11-30 00:59:54,211 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-11-30 00:59:54,221 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-30 00:59:54,225 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-30 00:59:54,232 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 00:59:54,233 : INFO : EPOCH - 4 : training on 10890638 raw words (9629111 effective words) took 5.4s, 1775839 effective words/s\n",
      "2021-11-30 00:59:55,242 : INFO : EPOCH 5 - PROGRESS: at 20.34% examples, 1935422 words/s, in_qsize 23, out_qsize 0\n",
      "2021-11-30 00:59:56,242 : INFO : EPOCH 5 - PROGRESS: at 40.15% examples, 1928232 words/s, in_qsize 22, out_qsize 1\n",
      "2021-11-30 00:59:57,254 : INFO : EPOCH 5 - PROGRESS: at 60.08% examples, 1921704 words/s, in_qsize 21, out_qsize 2\n",
      "2021-11-30 00:59:58,255 : INFO : EPOCH 5 - PROGRESS: at 79.66% examples, 1909399 words/s, in_qsize 24, out_qsize 0\n",
      "2021-11-30 00:59:59,258 : INFO : EPOCH 5 - PROGRESS: at 98.18% examples, 1884201 words/s, in_qsize 23, out_qsize 0\n",
      "2021-11-30 00:59:59,329 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-11-30 00:59:59,333 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-11-30 00:59:59,335 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-11-30 00:59:59,345 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-11-30 00:59:59,347 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-11-30 00:59:59,348 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-11-30 00:59:59,348 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-11-30 00:59:59,349 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-11-30 00:59:59,350 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-11-30 00:59:59,351 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-30 00:59:59,361 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-30 00:59:59,366 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-30 00:59:59,367 : INFO : EPOCH - 5 : training on 10890638 raw words (9629631 effective words) took 5.1s, 1878189 effective words/s\n",
      "2021-11-30 00:59:59,367 : INFO : training on a 54453190 raw words (48146713 effective words) took 26.1s, 1847750 effective words/s\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(data, min_count = 0, workers = cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', 0.7683312892913818),\n",
       " ('luck', 0.6691802740097046),\n",
       " ('everyone', 0.6319742798805237),\n",
       " ('excellent', 0.6215455532073975),\n",
       " ('feel', 0.6146776676177979),\n",
       " ('know', 0.6136137247085571),\n",
       " ('pretty', 0.6117483973503113),\n",
       " ('easy', 0.6068543195724487),\n",
       " ('everybody', 0.6020517349243164),\n",
       " ('things', 0.6010101437568665)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando e Avaliando os Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class question_word:\n",
    "    \n",
    "    def __init__(self, line):\n",
    "        self.w1 = line.split(' ')[0].lower()\n",
    "        self.w2 = line.split(' ')[1].lower()\n",
    "        self.w3 = line.split(' ')[2].lower()\n",
    "        self.match = line.split(' ')[3].replace('\\n','').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Words : 19544\n"
     ]
    }
   ],
   "source": [
    "dict_question_words = {}\n",
    "\n",
    "with open(\"data/questions-words.txt\", 'r') as file:\n",
    "    all_lines = file.readlines()\n",
    "    \n",
    "words_to_evaluate = [question_word(ln) for ln in all_lines if ':' not in ln]\n",
    "print(\"All Words :\", len(words_to_evaluate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_question_words(model, words, vocab):\n",
    "    w_error = []\n",
    "    count_not_in_vocab = 0\n",
    "    \n",
    "    for question in words:\n",
    "        #print(question.w1, question.w2, question.w3)\n",
    "        \n",
    "        if question.w1 in vocab and question.w2 in vocab and question.w3 in vocab and question.match in vocab:\n",
    "        \n",
    "            result = model.wv.most_similar(positive=[question.w1, question.w2], negative=[question.w3], topn=10)   \n",
    "            nearest_word = result[0]\n",
    "\n",
    "            if nearest_word[0] != question.match:\n",
    "                w_error.append(model.wv.similarity(nearest_word[0], question.match))\n",
    "            else:\n",
    "                w_error.append(0)\n",
    "        else:\n",
    "            w_error.append(1)\n",
    "            count_not_in_vocab += 1\n",
    "            \n",
    "    mean = np.average(w_error)\n",
    "    std = np.std(w_error)\n",
    "    var = np.var(w_error)\n",
    "    \n",
    "    return mean, std, var, count_not_in_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38362901163775226 0.2834418531407765 0.08033928411187752 644\n"
     ]
    }
   ],
   "source": [
    "mean_error, std_error, std_var, w_not_vocab = get_error_question_words(w2v_model, words_to_evaluate, w2v_model.wv)\n",
    "print(mean_error, std_error, std_var, w_not_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 01:20:13,627 : INFO : saving Word2Vec object under saved_models/w2v.model, separately None\n",
      "2021-11-30 01:20:13,628 : INFO : storing np array 'vectors' to saved_models/w2v.model.wv.vectors.npy\n",
      "2021-11-30 01:20:13,687 : INFO : not storing attribute vectors_norm\n",
      "2021-11-30 01:20:13,688 : INFO : storing np array 'syn1neg' to saved_models/w2v.model.trainables.syn1neg.npy\n",
      "2021-11-30 01:20:14,075 : INFO : saved saved_models/w2v.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model.save(\"saved_models/w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 01:20:50,531 : INFO : loading Word2Vec object from saved_models/w2v.model\n",
      "2021-11-30 01:20:51,109 : INFO : loading wv recursively from saved_models/w2v.model.wv.* with mmap=None\n",
      "2021-11-30 01:20:51,109 : INFO : loading vectors from saved_models/w2v.model.wv.vectors.npy with mmap=None\n",
      "2021-11-30 01:20:51,153 : INFO : setting ignored attribute vectors_norm to None\n",
      "2021-11-30 01:20:51,153 : INFO : loading vocabulary recursively from saved_models/w2v.model.vocabulary.* with mmap=None\n",
      "2021-11-30 01:20:51,154 : INFO : loading trainables recursively from saved_models/w2v.model.trainables.* with mmap=None\n",
      "2021-11-30 01:20:51,154 : INFO : loading syn1neg from saved_models/w2v.model.trainables.syn1neg.npy with mmap=None\n",
      "2021-11-30 01:20:51,195 : INFO : loaded saved_models/w2v.model\n"
     ]
    }
   ],
   "source": [
    "m = Word2Vec.load('saved_models/w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "text = \"\"\n",
    "with open(\"data/text8\", 'r') as infile:\n",
    "    for line in infile:\n",
    "        text = process_text(line)\n",
    "        \n",
    "        count = + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated astermabuse first usedearly \n"
     ]
    }
   ],
   "source": [
    "print(text[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = word2vec.Text8Corpus('data/text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bad', 0.7443740963935852),\n",
       " ('poor', 0.5743762254714966),\n",
       " ('quick', 0.5488086342811584),\n",
       " ('reasonable', 0.5429285764694214),\n",
       " ('safe', 0.5324724912643433),\n",
       " ('little', 0.5284340381622314),\n",
       " ('clever', 0.5271481275558472),\n",
       " ('wrong', 0.5259556770324707),\n",
       " ('fun', 0.5209583044052124),\n",
       " ('simple', 0.5187517404556274)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kiev', 0.4300565719604492),\n",
       " ('carta', 0.4149296283721924),\n",
       " ('brabant', 0.4144781827926636),\n",
       " ('umayyad', 0.40684974193573),\n",
       " ('iii', 0.39912277460098267),\n",
       " ('ducal', 0.394608736038208),\n",
       " ('cordoba', 0.39357495307922363),\n",
       " ('hohenstaufen', 0.39350447058677673),\n",
       " ('abdicates', 0.3897632956504822),\n",
       " ('achaemenid', 0.38964521884918213)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(negative=['man', 'women'], positive=['king'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66290057"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('king', 'queen')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
